# Retail-Data-Pipeline-GCP
# Airflow_ETL

ğŸš€ ETL Automation with Apache Airflow

ğŸ“Œ Overview

This project is part of the Retail Data Pipeline (GCP) and focuses on automating ETL workflows using Apache Airflow to schedule and manage data ingestion, transformation, and storage in Google BigQuery.

The goal is to:
âœ… Schedule and automate ETL jobs for retail sales data
âœ… Integrate Google Cloud Storage (GCS) â†’ BigQuery
âœ… Monitor & manage workflows using Apache Airflow DAGs

ğŸ”§ Technologies Used

Apache Airflow â€“ Workflow orchestration & automation

Google Cloud Storage (GCS) â€“ Raw data staging

Google BigQuery â€“ Data warehousing

Python â€“ Scripting Airflow DAGs

ğŸ“‚ Project Structure
```
airflow_etl/
â”‚â”€â”€ dags/              # Airflow DAGs for automating ETL
â”‚â”€â”€ scripts/           # Python scripts for automation
â”‚â”€â”€ README.md          # Documentation for ETL automation
```
ğŸ›  Steps to Recreate

1ï¸âƒ£ Set Up Apache Airflow

ğŸ“Œ Local Installation (For Testing)

pip install apache-airflow
airflow db init
airflow webserver --port 8080
airflow scheduler

ğŸ“Œ Google Cloud Composer (For Production)

Go to Google Cloud Console â†’ Cloud Composer

Set up a managed Apache Airflow environment

2ï¸âƒ£ Develop an ETL DAG in Airflow

Create a new DAG file: dags/retail_etl_dag.py

from airflow import DAG
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryExecuteQueryOperator
from airflow.utils.dates import days_ago

default_args = {
    "owner": "airflow",
    "start_date": days_ago(1),
    "depends_on_past": False,
}

dag = DAG(
    "retail_sales_etl",
    default_args=default_args,
    schedule_interval="@daily",
)

load_to_bq = GCSToBigQueryOperator(
    task_id="load_data",
    bucket="adventureworks-data",
    source_objects=["sales_order_header.csv"],
    destination_project_dataset_table="retail_data_pipeline.sales_order_header",
    autodetect=True,
    write_disposition="WRITE_TRUNCATE",
    dag=dag,
)

load_to_bq

3ï¸âƒ£ Deploy & Run Airflow DAG

Upload retail_etl_dag.py to dags/ folder in Airflow

Restart Airflow:

airflow scheduler
airflow webserver

Go to Airflow UI (http://localhost:8080), enable and trigger the DAG

âœ… Now the ETL workflow runs automatically!

ğŸ“Š Monitoring & Performance Optimization

Airflow UI DAG Runs â†’ Check for task failures

Enable email alerts for failures:

default_args = {
    "owner": "airflow",
    "start_date": days_ago(1),
    "email_on_failure": True,
    "email": ["your-email@example.com"]
}

Optimize BigQuery queries for performance tuning

ğŸ”œ Next Steps & Improvements

Implement incremental data updates instead of full loads

Integrate data validation checks before processing

Migrate DAGs to Google Cloud Composer for scalability

ğŸ”— Links & Resources

ğŸ“‚ GitHub Repo: Retail Data Pipeline GCP

ğŸ“„ Google Cloud Composer Docs: Read Here

ğŸ“Œ How to Use:

Clone this repo: git clone https://github.com/jessejoosse/Retail-Data-Pipeline-GCP.git

Follow these steps to set up ETL automation with Airflow

ğŸš€ Built for scalable, automated cloud data pipelines!

